# 应用统计学复习文档

## 一、数理统计基础

### 1. 基本概念
- **总体**：研究对象的全体
- **样本**：从总体中抽取的部分个体
- **统计量**：样本的函数，用于估计总体参数

### 2. 常见统计量
- **样本均值**：$\bar{x} = \frac{1}{n}\sum_{i=1}^{n}x_i$
- **样本方差**：$s^2 = \frac{1}{n-1}\sum_{i=1}^{n}(x_i-\bar{x})^2$
- **样本k阶原点矩**：$m_k = \frac{1}{n}\sum_{i=1}^{n}x_i^k$
- **样本k阶中心矩**：$m_k = \frac{1}{n}\sum_{i=1}^{n}(x_i-\bar{x})^k$

### 3. 样本均值和方差的性质
- **样本均值的期望**：$E(\bar{X}) = \mu$（无偏估计）
- **样本均值的方差**：$Var(\bar{X}) = \frac{\sigma^2}{n}$
- **样本方差的期望**：$E(S^2) = \sigma^2$（无偏估计）

### 4. 重要分布
- **卡方分布**：若$Z_1, Z_2, ..., Z_n$为独立的标准正态随机变量，则$\sum_{i=1}^{n}Z_i^2 \sim \chi^2(n)$
  - 性质：均值为n，方差为2n
  - 形状：右偏分布，随自由度增加趋近于正态分布
  
- **t分布**：若$Z \sim N(0,1)$，$V \sim \chi^2(n)$且Z与V独立，则$T = \frac{Z}{\sqrt{V/n}} \sim t(n)$
  - 性质：对称分布，均值为0，方差为$\frac{n}{n-2}$（当n>2）
  - 形状：钟形，比正态分布尾部更厚，随自由度增加趋近于标准正态分布
  
- **F分布**：若$U \sim \chi^2(m)$，$V \sim \chi^2(n)$且U与V独立，则$F = \frac{U/m}{V/n} \sim F(m,n)$
  - 性质：右偏分布，均值为$\frac{n}{n-2}$（当n>2）
  - 应用：方差分析和回归分析中的显著性检验

### 5. 点估计方法

- **矩估计法**：
  - 基本思想：用样本矩估计总体矩，再求解参数
  - 步骤：
    1. 建立总体矩与参数的关系
    2. 用样本矩替代总体矩
    3. 解方程得到参数估计值
  - 优点：计算简单
  - 缺点：效率可能不高，不一定是最优估计

- **极大似然估计法**：
  - 基本思想：选择能使观测数据出现概率最大的参数值
  - 步骤：
    1. 构建似然函数$L(\theta) = f(x_1,x_2,...,x_n|\theta)$
    2. 取对数得到对数似然函数
    3. 求导数并令其为0，解方程得到参数估计值
  - 优点：大样本下具有良好的统计性质（一致性、渐近正态性、渐近有效性）
  - 缺点：计算可能复杂
  - 特殊情况：均匀分布的极大似然估计比较特殊，通常取样本的最大值作为参数估计

## 二、线性回归模型

### 1. 一元线性回归

- **模型定义**：$Y_i = \beta_0 + \beta_1 X_i + \varepsilon_i$，其中$\varepsilon_i \sim N(0, \sigma^2)$
- **矩阵表示**：$\mathbf{Y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon}$
- **基本假设**：
  - 线性关系
  - 误差项独立同分布
  - 误差项服从正态分布
  - 误差项与自变量不相关
  - 误差项具有同方差性

- **参数估计**：
  - 最小二乘估计：$\hat{\beta}_1 = \frac{\sum(X_i-\bar{X})(Y_i-\bar{Y})}{\sum(X_i-\bar{X})^2}$，$\hat{\beta}_0 = \bar{Y} - \hat{\beta}_1\bar{X}$
  - 误差方差估计：$\hat{\sigma}^2 = \frac{1}{n-2}\sum(Y_i - \hat{Y}_i)^2$
  - 极大似然估计：与最小二乘估计结果相同

- **预测**：
  - 点预测：$\hat{Y}_0 = \hat{\beta}_0 + \hat{\beta}_1 X_0$
  - 预测区间考虑了新观测值的随机性

### 2. 多元线性回归

- **模型定义**：$Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + ... + \beta_p X_{ip} + \varepsilon_i$
- **矩阵表示**：$\mathbf{Y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon}$
- **参数估计**：
  - 最小二乘估计：$\hat{\boldsymbol{\beta}} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y}$
  - 误差方差估计：$\hat{\sigma}^2 = \frac{1}{n-p-1}\sum(Y_i - \hat{Y}_i)^2$

### 3. 模型评估

- **决定系数**：$R^2 = 1 - \frac{SSE}{SST} = \frac{SSR}{SST}$
  - SSE：残差平方和
  - SST：总平方和
  - SSR：回归平方和
  - 含义：自变量解释因变量变异的比例
  - 范围：0到1，越接近1表示拟合越好

- **调整的决定系数**：$R_{adj}^2 = 1 - \frac{SSE/(n-p-1)}{SST/(n-1)}$
  - 考虑了自变量个数的影响
  - 惩罚过多的自变量
  - 适用于比较不同自变量个数的模型

- **自由度**：
  - 总自由度：n-1
  - 回归自由度：p
  - 残差自由度：n-p-1

### 4. 回归诊断

- **多重共线性**：
  - 定义：自变量之间存在高度相关性
  - 判断方法：
    - 相关系数矩阵
    - 方差膨胀因子(VIF)：$VIF_j = \frac{1}{1-R_j^2}$，其中$R_j^2$是第j个自变量对其他自变量的回归决定系数
    - 条件数：设计矩阵特征值的最大值与最小值之比
  - 解决方法：
    - 删除高度相关的变量
    - 增加样本量
    - 变量标准化
    - 岭回归：添加正则化项$\lambda\sum\beta_j^2$
    - 岭迹法：绘制岭迹图选择合适的调整参数$\lambda$

- **异方差性**：
  - 定义：误差项方差不恒定
  - 判断方法：
    - 残差图呈现"漏斗型"
    - 布雷什-帕甘检验
  - 解决方法：
    - 变量变换
    - 加权最小二乘法(WLS)
    - Box-Cox变换

- **残差分析**：
  - 普通残差：$e_i = Y_i - \hat{Y}_i$
  - 标准化残差：$r_i = \frac{e_i}{\hat{\sigma}\sqrt{1-h_{ii}}}$
  - 外学生化残差：使用剔除第i个观测值后的方差估计
  - 杠杆统计量($h_{ii}$)：衡量观测点对拟合值的影响程度

- **影响分析**：
  - 异常点：残差较大的观测值
  - DFFITS准则：衡量删除某观测值对拟合值的影响
  - Cook距离：衡量删除某观测值对所有回归系数的综合影响
  - COVRATIO准则：衡量删除某观测值对协方差矩阵的影响

## 三、重抽样方法

### 1. 基本思想
重抽样方法通过反复从原始样本中抽取样本来估计统计量的分布或评估模型性能。

### 2. 交叉验证法

- **验证集方法**：
  - 将数据分为训练集和验证集
  - 用训练集拟合模型，用验证集评估性能
  - 缺点：结果依赖于数据划分方式

- **留一交叉验证法(LOOCV)**：
  - 每次留出一个观测值作为验证集
  - 需要拟合n个模型
  - 优点：充分利用数据
  - 缺点：计算量大

- **k折交叉验证法**：
  - 将数据随机分为k个大小相近的子集
  - 每次用k-1个子集训练，剩余1个子集验证
  - 重复k次，取平均误差
  - 常用k=5或k=10

- **广义交叉验证法(GCV)**：
  - 是LOOCV的近似计算方法
  - 适用于某些特定模型，如线性模型和平滑样条

### 3. 自助法(Bootstrap)

- **基本原理**：从原始样本中有放回地抽取样本
- **执行步骤**：
  1. 从原始样本中有放回地抽取n个观测值，形成自助样本
  2. 基于自助样本计算统计量
  3. 重复步骤1和2多次(如B=1000次)
  4. 基于B个统计量估计其分布、标准误等

- **应用**：
  - 估计统计量的标准误
  - 构建置信区间
  - 假设检验
  - 模型选择

## 四、模型选择与正则化

### 1. 传统子集选择方法

- **最优子集选择**：
  - 尝试所有可能的变量组合
  - 选择最优的子集
  - 计算量大，不适用于高维数据

- **逐步选择方法**：
  - 向前逐步选择：从空模型开始，逐步添加变量
  - 向后逐步选择：从全模型开始，逐步删除变量
  - 逐步回归：结合向前和向后的方法

- **选择标准**：
  - 调整的决定系数
  - Cp准则：$C_p = \frac{SSE_p}{\hat{\sigma}^2} - (n-2p)$
  - 信息准则：
    - AIC(赤池信息准则)：$AIC = -2\ln(L) + 2p$
    - BIC(贝叶斯信息准则)：$BIC = -2\ln(L) + p\ln(n)$

### 2. 正则化方法

- **岭回归**：
  - 目标函数：$\min_{\beta} \|Y-X\beta\|^2 + \lambda\|\beta\|^2_2$
  - 添加L2正则化项
  - 收缩系数但不产生稀疏解
  - 适用于处理多重共线性

- **Lasso回归**：
  - 目标函数：$\min_{\beta} \|Y-X\beta\|^2 + \lambda\|\beta\|_1$
  - 添加L1正则化项
  - 产生稀疏解，实现变量选择
  - 适用于高维数据

- **弹性网络**：
  - 结合L1和L2正则化
  - 目标函数：$\min_{\beta} \|Y-X\beta\|^2 + \lambda_1\|\beta\|_1 + \lambda_2\|\beta\|^2_2$
  - 兼具Lasso和岭回归的优点

- **桥回归**：
  - 目标函数：$\min_{\beta} \|Y-X\beta\|^2 + \lambda\sum_{j=1}^p|\beta_j|^q$
  - q=1时为Lasso，q=2时为岭回归
  - 可调整q值以平衡稀疏性和收缩效果

- **SCAD(平滑削减绝对偏差)**：
  - 非凸惩罚函数
  - 满足无偏性、稀疏性和连续性
  - 对大系数的惩罚较小

- **自适应Lasso**：
  - 为不同系数分配不同的惩罚权重
  - 权重通常基于初始估计(如OLS)的倒数
  - 具有Oracle性质

## 五、非参数回归模型

### 1. 基本概念

- **定义**：不预设具体函数形式的回归模型
- **优点**：
  - 灵活性强，适应各种函数形式
  - 不依赖于参数分布假设
- **缺点**：
  - 需要更多的样本量
  - 计算复杂度高
  - 解释性较差

### 2. 常用方法

- **多项式回归**：
  - 使用多项式函数拟合数据
  - 形式：$f(x) = \beta_0 + \beta_1 x + \beta_2 x^2 + ... + \beta_d x^d$
  - 优点：简单易实现
  - 缺点：高阶多项式可能过拟合，全局性质导致局部拟合不佳

- **分段多项式拟合**：
  - 在不同区间使用不同的多项式函数
  - 提高了局部拟合能力
  - 但在节点处可能不光滑

- **回归样条**：
  - d阶回归样条：由d阶分段多项式构成，在节点处具有d-1阶连续导数
  - 线性样条：一阶样条，在节点处连续
  - 三次样条：三阶样条，在节点处具有二阶连续导数
  - 自然三次样条：在边界外施加线性约束的三次样条

- **样条节点选择**：
  - 等间距方法
  - 等间距样本分位数方法
  - 变量选择方法

- **光滑样条**：
  - 添加光滑度惩罚项的样条
  - 自动选择最优的光滑参数

- **局部非参数光滑方法**：
  - Nadaraya-Watson核光滑方法：使用核函数对局部数据加权平均
  - 局部多项式光滑方法：在每个点附近拟合低阶多项式

## 六、Logistic回归

### 1. 基本概念

- **二元Logistic回归**：
  - 模型：$P(Y=1|X) = \frac{e^{\beta_0 + \beta_1 X_1 + ... + \beta_p X_p}}{1 + e^{\beta_0 + \beta_1 X_1 + ... + \beta_p X_p}}$
  - 对数几率：$\ln\frac{P(Y=1|X)}{1-P(Y=1|X)} = \beta_0 + \beta_1 X_1 + ... + \beta_p X_p$
  - 适用于二分类问题

- **参数估计**：
  - 极大似然估计法
  - 迭代加权最小二乘法

### 2. 模型评估

- **混淆矩阵**：
  - 真正例(TP)：实际为正，预测为正
  - 假正例(FP)：实际为负，预测为正
  - 真负例(TN)：实际为负，预测为负
  - 假负例(FN)：实际为正，预测为负

- **评估指标**：
  - 灵敏度(敏感性)：$\frac{TP}{TP+FN}$，正确识别正例的比例
  - 特异度：$\frac{TN}{TN+FP}$，正确识别负例的比例
  - 精确率：$\frac{TP}{TP+FP}$，预测为正的样本中实际为正的比例
  - 召回率：等同于灵敏度
  - ROC曲线：以1-特异度为横坐标，灵敏度为纵坐标的曲线
  - AUC：ROC曲线下面积，越接近1表示模型越好

### 3. 扩展

- **惩罚似然变量选择**：
  - 在对数似然函数中添加惩罚项
  - 类似于线性回归中的正则化方法

- **非参数Logistic回归**：
  - 将线性预测子替换为非参数函数
  - 增加模型灵活性

- **多项Logistic回归**：
  - 处理多分类问题
  - 为每个类别(除参考类别外)建立一个对数几率模型
  - 参数估计同样使用极大似然法
