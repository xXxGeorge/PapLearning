### 概述

迁移学习, 是样本量不足或样本量远少于参数量的时候用辅助性样本来提高模型表现的一种方法, 这个时候直接进行回归分析会导致方差过大等不良结果. 我们不妨称呼这个这个缺少样本的模型为A. 所以我们寻找辅助样本(称呼为B), 利用系数的稀疏性($l_q - sparsity$)来判断样本能否用于迁移学习, 并将原本的求A的回归模型的问题转化: 1.为求出B 的回归模型. 2. 加上B与A的误差项. 最终得到对 A 的回归模型. 其中分为两种情况, 我们是否知道哪些集合是informative auxiliary set, 对于不知道的情况我们会转化为知道的情况并解决

Poisson 回归, 是一种基于 Poisson 分布的一种回归. 他对因变量的均值进行建模, 也就是对于 Y 所符合的 Poisson 分布的强度 $\lambda$ 进行建模, 认为 $\lambda$ 和自变量的线性组合之间以对数关系链接. 

地理加权 Poisson 回归, 是一种基于 GWR, 同时结合了 Poisson 回归的回归模型. 我们在原本的 Poisson 回归的基础上加入 GWR 的特征, 使得每一个参数都成为一个与取值点位置有关的函数, 使模型可以反应地理位置对于因变量的影响.

空间自回归模型, 与 GWR 相比的话, GWR 侧重模型在不同地理位置时候的不同参数取值, 而 SAR 模型只有一套参数值, 但是他的模型中含有空间自回归项, 可以体现周围的地区的因变量变化对于目标地区的影响.

### Transfer Learning

在处理高维线性回归问题时，当目标任务的样本数量不足以直接进行精确建模时，迁移学习提供了一种有效的解决方案。本节将详细探讨迁移学习在线性回归中的应用，特别是针对样本量不足或参数量远大于样本量的情况。

#### 问题背景

考虑一个线性回归模型，其形式可以表示为矩阵形式：
$$y = X\beta + \epsilon$$
其中，$y$ 是响应变量向量，$X$ 是设计矩阵，$\beta$ 是回归系数向量，$\epsilon$ 是误差项。当目标任务（我们称之为任务 A）的样本数量 $n_A$ 非常小，甚至远小于特征维度 $p$ 时，直接使用最小二乘法或其他传统方法估计 $\beta$ 会导致方差过大、过拟合等问题，模型泛化能力差。

为了解决这个问题，我们可以引入来自相关研究的辅助数据集。假设我们有 $K$ 个辅助任务，每个任务 $k$ 包含 $n_k$ 个样本，其模型可以表示为：
$$y_i^k = (x_i^k)^Tw^k + \epsilon_i^k$$
其中，$y_i^k$ 是第 $k$ 个任务的第 $i$ 个响应变量，$x_i^k$ 是对应的特征向量，$w^k$ 是第 $k$ 个任务的回归系数向量，$\epsilon_i^k$ 是误差项。

#### 信息性辅助样本的识别

迁移学习的关键在于识别哪些辅助任务是“信息性”的，即它们的回归系数 $w^k$ 与目标任务的回归系数 $\beta$ 足够接近。我们通过 $l_q$-稀疏性来判断这种接近程度。具体地，我们构造一个集合 $\mathbb A_q$，包含所有满足以下条件的辅助任务 $k$：
$$\mathbb A_q = \{1 \leq k \leq K, \quad  ||w^{(k)} - \beta||_q \leq h\}$$
其中，$||\cdot||_q$ 表示 $l_q$ 范数，$h$ 是一个预设的阈值。这个定义意味着，如果辅助任务 $k$ 的系数 $w^{(k)}$ 与目标任务的系数 $\beta$ 在 $l_q$ 范数意义下的距离小于 $h$，则认为该辅助任务是信息性的。
##### Question
然而，这里存在一个核心问题：我们如何在不知道 $\beta$ 的取值的情况情况下, 使用这个准则来判断稀疏性
##### Answer;
我猜测可能是因为公式中是用每一个样本去计算的，这些样本显然可以计算出结果，只是容易过拟合，方差大，所以这个方法真正的第一步其实应该是先对每一个样本点计算一个 $\beta$ 出来，然后再对他进行进一步的修正。(但这种解释不一定对?)，因为小样本下参数估计的方差很大，可能导致原本不稀疏的辅助性样本被错误地选入，而稀疏的反而被淘汰。

查询资料得知，在不知道 $\beta$ 的真实值时，通常会采用一些启发式方法或两阶段方法来近似识别信息性辅助任务。例如，可以先对每个辅助任务独立进行初步估计得到 $\hat{w}^{(k)}$，然后利用这些初步估计值来判断其与目标任务的潜在相关性。更严谨的方法可能涉及交叉验证或正则化技术来选择最相关的辅助任务，或者在优化过程中同时估计 $\beta$ 和辅助任务的权重。

#### 迁移学习的两种优化情况

一旦我们识别出信息性辅助样本，就可以分两种情况对模型进行优化：

##### Oracle Trans-Lasso

在这种理想情况下，我们假设已经已知哪些集合是信息性辅助集 $\mathbb A_q$。在这种设定下，我们可以采用以下算法来估计目标任务的回归系数 $\beta$：

**Step 1: 估计辅助任务的基准系数 $\hat{\omega}$**

我们首先构造一个含有 $L_1$ 正则项的优化函数（通常取 $q=1$，即 $L_1$ 范数，以促进稀疏性），对信息性辅助任务的系数进行估计：
$$\hat \omega = arg\, min\{\frac{1}{2n_A}\sum_{k \in A_q}||y^k - x^k\omega^k||_2^2 +  \lambda_{\omega}||\omega^k||\}$$
其中，$n_A$ 是信息性辅助任务的总样本数，$\lambda_{\omega} = c_1 \sqrt{\frac{\log p}{n_A}}$ 是正则化参数，$c_1$ 是一个常数。这一步旨在从信息性辅助任务中学习一个共享的、稀疏的基准系数 $\hat{\omega}$。

**Step 2: 修正基准系数以得到目标任务的精确估计 $\hat{\beta}$**

在得到基准系数 $\hat{\omega}$ 后，我们对其进行修正，以更精确地估计目标任务的系数 $\hat{\beta}$。这种修正通常通过引入一个误差项 $\hat{\epsilon}$ 来实现，使得 $\hat{\beta} = \hat{\omega} + \hat{\epsilon}$。误差项 $\hat{\epsilon}$ 的估计通过以下优化问题得到：
$$\hat \epsilon = \arg \,\, \min_{\epsilon} \left\{\frac{1}{2n_A}\sum_{k \in \mathbb A_q}||y^k - x^k\hat{\omega}  - x^k \epsilon^k||_2^2 + \lambda_{\omega} ||\hat{\omega}||_1 + \lambda_{\epsilon}||\epsilon^k||_1\right\}$$
这里，$\lambda_{\epsilon}$ 是误差项的正则化参数。通过这种两阶段的优化，我们能够利用辅助任务的信息来稳定对目标任务系数的估计，尤其是在目标任务数据量有限的情况下。

##### Trans-Lasso

与 Oracle Trans-Lasso 不同，Trans-Lasso 适用于我们**不知道**哪些辅助任务是信息性辅助集的情况。在这种更实际的场景中，我们需要设计一种机制来同时识别信息性辅助任务并估计目标任务的系数。

Trans-Lasso 的核心思想是计算多个估计器（estimators），然后以某种方式聚合它们，最终构造出一个模型。理论上，这个模型不会比已知稀疏度时（即 Oracle Trans-Lasso）得到的模型更差。这通常通过引入一个权重选择机制来实现，该机制根据辅助任务与目标任务的相似性（例如，通过交叉验证或启发式度量）来分配不同的权重。具体实现可能涉及迭代算法，在每次迭代中更新辅助任务的权重和目标任务的系数估计，直到收敛。

这种方法在实践中更具挑战性，因为它需要有效地处理辅助任务的选择和权重分配，以避免引入噪声或不相关的辅助信息。然而，它也更具普适性，因为它不需要预先知道哪些辅助任务是信息性的。

## Poisson Regression

### Poisson分布的基本概念

Poisson回归是建立在Poisson分布的基础上的，故首先先简要了解一下Poisson分布。
#### Definition： Poisson Distribution

如果随机变量 $X$ 的取值是非负整数（$k = 0, 1, 2, \dots$），且其PMF为：
$$P(X=k)=\frac{\lambda^k}{k!}e^{-\lambda}, \quad k = 0, 1, 2, \dots$$
其中，$\lambda > 0$ 是 Poisson 分布的参数，表示在给定时间或空间间隔内事件发生的平均次数。则称 $X$ 服从参数为 $\lambda$ 的 Poisson 分布，记作 $X \sim Poisson(\lambda)$。

Poisson 分布具有一个重要性质：其均值和方差都等于参数 $\lambda$(同分散性)，即 $E(X) = Var(X) = \lambda$。这表明 Poisson 分布是一种特殊的计数模型，适用于描述稀有事件在一定区间内发生的次数。

#### Definition：Poisson Regression Model

Poisson 回归模型用于分析计数数据（非负整数）与一组预测变量之间的关系。其核心思想是，响应变量 $Y$ 服从 Poisson 分布，且其均值（即 Poisson 分布的参数 $\lambda$）与预测变量 $X$ 之间存在对数线性关系。具体地，模型形式如下：

$$\displaylines {\ln(\lambda(X))= \beta_0 + \sum_{j = 1}^p\beta_jX_j \\ \Rightarrow \lambda(X) = \exp(\beta_0 + \sum_{j = 1}^p\beta_jX_j)}$$

其中，$\lambda(X)$ 是在给定预测变量 $X = (X_1, X_2, \dots, X_p)$ 条件下，响应变量 $Y$ 的条件均值。$\beta_0, \beta_1, \dots, \beta_p$ 是回归系数。因此，在给定 $X$ 的情况下，$Y$ 的概率质量函数为：

$$P(Y = y| X) = \frac{\lambda(X)^y e^{-\lambda(X)}}{y!}, \quad y = 0, 1, 2, \dots$$

Poisson Regression 的实际意义在于，它能够量化预测变量对事件发生率（或计数）的影响。例如：
- **$Y$**：某条路段一周内发生的事故次数
- **$X$**：限速、车流量、路段长度、是否有红绿灯等

尽管有 $X$ 的影响，但事故的发生仍然具有随机性，且事故次数是计数数据，符合 Poisson 分布的特点。

我们会发现 Poisson 回归的形式与我们之前学习的有一点不同: 上面两种形式没有写出显式的 $\hat Y$ ,而是以概率形式出现. 但事实上,我们已经知道 $\lambda$ 是 Y 的均值, 而这个 $\lambda$ 又是在 X 的前提下得到的, 也就是说我们可以知道此时 Y 的条件期望 $E(Y|X) = exp(\beta_0 + \sum_{i =1}^p\beta_iX_i)$ ,这也正是 $\hat Y$.

#### Question

为什么我们对 $\lambda$ 建模的是 $\lambda$ 本身而不是 $E(\lambda)$?

#### Answer

这个问题的核心在于理解 Poisson 分布的随机性来源。在 Poisson 回归中，我们假设响应变量 $Y$ 在给定预测变量 $X$ 的条件下精确地服从参数为 $\lambda(X)$ 的 Poisson 分布。这里的 $\lambda(X)$ 是一个确定的值，由预测变量 $X$ 和回归系数 $\beta$ 唯一确定。事件 $Y$ 的随机性完全来源于 Poisson 分布本身的随机过程，即在已知平均发生率 $\lambda(X)$ 的情况下，实际发生次数 $Y$ 的波动。

如果我们将模型写成 $E(\lambda)$，则意味着 $\lambda$ 本身是一个随机变量，其取值过程中存在随机误差 $\epsilon$。但这与 Poisson 回归的假设不符。在 Poisson 回归中，一旦确定了 $X$ 和 $\beta$，$\lambda(X)$ 就是一个固定的、非随机的强度参数，它决定了 Poisson 过程的平均发生率。因此，我们直接对这个确定性的强度参数 $\lambda(X)$ 进行建模，而不是对其期望进行建模。

### 参数估计方法

Poisson 回归模型的参数 $\beta = (\beta_0, \beta_1, \dots, \beta_p)^T$ 通常采用 MLE 方法进行估计。给定一组独立的观测数据 $(y_i, x_i)$，其中 $i=1, \dots, n$，似然函数可以写为各个观测值概率的乘积：

$$L(\beta) = \prod_{i=1}^n P(Y_i = y_i | X_i) = \prod_{i=1}^n \frac{\lambda(X_i)^{y_i} e^{-\lambda(X_i)}}{y_i!}$$

为了方便计算，我们通常使用对数似然函数：

$$\ell(\beta) = \ln L(\beta) = \sum_{i=1}^n \left[ y_i \ln(\lambda(X_i)) - \lambda(X_i) - \ln(y_i!) \right]$$

将 $\ln(\lambda(X_i)) = \beta_0 + \sum_{j = 1}^p\beta_jX_{ij}$ 代入，得到：

$$\ell(\beta) = \sum_{i=1}^n \left[ y_i (\beta_0 + \sum_{j = 1}^p\beta_jX_{ij}) - \exp(\beta_0 + \sum_{j = 1}^p\beta_jX_{ij}) - \ln(y_i!) \right]$$

最大似然估计的目标是找到使对数似然函数最大化的 $\beta$ 值。由于对数似然函数通常是非线性的，无法得到解析解，因此需要采用迭代优化算法来求解。常用的方法包括：

1.  **Newton-Raphson 方法**：
    这是一种常用的迭代优化算法，通过在当前估计值处计算对数似然函数的一阶导数（梯度）和二阶导数（Hessian 矩阵）来更新参数。迭代公式为：
    $$\beta^{(t+1)} = \beta^{(t)} - [H(\beta^{(t)})]^{-1} \nabla \ell(\beta^{(t)})$$
    其中，$\nabla \ell(\beta^{(t)})$ 是对数似然函数在 $\beta^{(t)}$ 处的梯度向量，$H(\beta^{(t)})$ 是 Hessian 矩阵。Newton-Raphson 方法收敛速度快，但需要计算和求逆 Hessian 矩阵，计算量较大。

2.  **迭代重加权最小二乘法（Iteratively Reweighted Least Squares, IRLS）**：
    IRLS 是一种更常用的方法，它将非线性的最大似然估计问题转化为一系列加权最小二乘问题。在每次迭代中，IRLS 会更新权重，然后使用加权最小二乘法来估计参数。IRLS 在广义线性模型（Generalized Linear Models, GLMs）中被广泛使用，而 Poisson 回归正是 GLM 的一个特例。IRLS 的优点是计算相对简单，且在许多情况下表现良好。

3.  **梯度下降法（Gradient Descent）及其变种**：
    虽然 Newton-Raphson 和 IRLS 更常用于 GLMs，但梯度下降法及其变种（如随机梯度下降 SGD、Adam 等）也可以用于 Poisson 回归的参数估计，尤其是在处理大规模数据集时。这些方法通过沿着梯度的反方向迭代更新参数来最小化负对数似然函数。

在实际应用中，许多统计软件（如 R、Python 的 `statsmodels` 库）都提供了内置函数来执行 Poisson 回归的参数估计，通常采用 IRLS 或类似的优化算法。

## Spatial Poisson Regression

### 空间数据的特征

在地理空间数据分析中，一个核心概念是**空间自相关性（Spatial Autocorrelation）**，它描述了地理位置上相近的观测值在数值上也趋于相似的现象。这种现象是空间数据分析区别于传统统计分析的关键特征之一。理解空间自相关性对于构建有效的空间模型至关重要。

#### 空间自相关性

空间自相关性是指某一地理区域的属性值与其相邻区域的属性值之间存在统计上的依赖关系。这种依赖关系可以是正向的（相似值聚集），也可以是负向的（相异值聚集），或者不存在（随机分布）。

1.  **正空间自相关（Positive Spatial Autocorrelation）**：
    -   邻近区域的观测值在数值上趋于相似。例如，高收入区域往往聚集在高收入区域附近，低收入区域聚集在低收入区域附近。这表现为“聚类”现象。
2.  **负空间自相关（Negative Spatial Autocorrelation）**：
    -   邻近区域的观测值在数值上趋于差异。例如，高收入区域可能被低收入区域包围，或者反之。这表现为“分散”或“交错”现象。
3.  **无空间自相关（No Spatial Autocorrelation / Random）**：
    -   观测值在空间上随机分布，区域间的数值没有明显的统计依赖关系。这表明空间位置对属性值没有显著影响。

#### Moran's I 指数

为了量化空间自相关性，常用的度量指标是 **Moran's I 指数**。Moran's I 用于反应空间邻接或空间临近的区域单元观测值整体的相关性和差异程度。它的计算依赖于空间权重矩阵的定义，不同的权重矩阵会影响 Moran's I 的值及其解释。

##### Global Moran's I

**Global Moran's I** 用于判断整个研究区域内**是否存在**空间聚集性或空间自相关性，是一个整体评估指标。它能够告诉我们空间模式是聚类、分散还是随机的，但不能指出具体哪个区域存在这种模式。

表达式：
设观测变量为 $y_i$，其均值为 $\bar{y}$，$w_{ij}$表示第$i$个和第$j$个地区的空间权重（例如：若邻接则为 1，否则为 0），总样本数为$n$。

$$I = \frac{n}{\sum_{i} \sum_{j} w_{ij}} \cdot \frac{\sum_{i} \sum_{j} w_{ij}(y_i - \bar{y})(y_j - \bar{y})}{\sum_{i} (y_i - \bar{y})^2}$$

Global Moran's I 的取值范围通常在 -1 到 1 之间：
-   接近 1 表示强烈的正空间自相关（聚类）。
-   接近 -1 表示强烈的负空间自相关（分散）。
-   接近 0 表示空间随机分布（无空间自相关）。

##### Local Moran's I

**Global Moran's I** 只能提供整体的空间趋势信息，但无法识别出具体哪个区域出现了聚集效应或异常值。为了弥补这一不足，我们使用 **Local Moran's I** 来识别“局部空间异常点/热点”。

**局部 Moran's I** 用于计算每个观测单位（例如，每个城市、每个地块）的空间自相关强度，从而揭示局部区域的聚类或异常模式。

表达式：
$$I_i = \frac{(y_i - \bar{y})}{\sum_k (y_k - \bar{y})^2} \cdot \sum_j w_{ij} (y_j - \bar{y})$$
这个 $I_i$ 表示第 $i$ 个单元与其邻居在该变量上的空间相关性。通过计算每个 $I_i$ 值，并结合其统计显著性，我们可以识别出不同类型的局部空间关联模式，例如高-高聚类（高值被高值包围）、低-低聚类（低值被低值包围）、高-低异常值（高值被低值包围）和低-高异常值（低值被高值包围）。

### 地理加权模型 GWR

地理加权回归允许回归系数随地理位置的变化而变化，从而捕捉空间异质性。与传统的全局回归模型（其回归系数在整个研究区域内保持不变）不同，GWR 能够为每个地理位置拟合一个局部回归模型。

| 概念                                     | 含义                                     | 与传统模型的区别                               | 关键特征                                   |
| :--------------------------------------- | :--------------------------------------- | :--------------------------------------------- | :----------------------------------------- |
| **Local Fitting (局部拟合)**             | 指的是 GWR 为每个地理位置拟合一个独立的回归模型。 | 传统模型只拟合一个全局模型。                   | 回归系数是地理位置的函数。                 |
| **Global Model (全局模型)**              | 指的是回归系数在整个研究区域内保持不变的模型。 | GWR 允许系数变化，全局模型则不允许。           | 假设变量关系在空间上是同质的。             |
| **Geographically Weighted Regression (GWR)** | 一种空间回归技术，通过对每个观测点进行加权最小二乘回归，使回归系数随空间位置变化。 | 能够捕捉空间异质性，提供更精细的空间分析结果。 | 核心在于“地理加权”，但其本质是“局部拟合”。 |

对于普通的多元回归，我们很难找到地理信息的关系，所以我们引入 GWR。对于局部模型，其形式是：
$$Y_i = \beta_{i0} + \sum_{k= 1}^pX_{ik}\beta_{ik} + \epsilon_i$$
其中，$Y_i$ 是在位置 $i$ 的响应变量，$X_{ik}$ 是在位置 $i$ 的第 $k$ 个预测变量，$\beta_{ik}$ 是在位置 $i$ 的第 $k$ 个回归系数，$\epsilon_i$ 是误差项。这里的 $\beta_{ik}$ 是一个在空间上变化的函数，即 $\beta_k(u_i, v_i)$，其中 $(u_i, v_i)$ 是位置 $i$ 的地理坐标。
##### Question
GWR 与独立局部拟合的区别：
这种形式与对每一个区域单独进行拟合有什么区别？形式上似乎没有体现出“加权”的特点。
##### Answer

经过进一步学习，我们了解到，GWR 并非简单地对每个区域单独拟合。如果仅仅对每个区域单独拟合，当区域内数据量较少时，拟合出来的模型会存在方差过大、不稳定等缺点。GWR 的核心思想是利用目标点周边的数据，并根据这些数据与目标点的空间距离或相关程度进行加权组合，从而提高有效数据量并稳定估计。这种加权是通过一个空间核函数来实现的，该函数根据距离赋予不同的权重，距离越近的观测点权重越大，距离越远的观测点权重越小。具体的权重数值由我们确定的 带宽决定。

因此，GWR 在估计某个位置 $(u_i, v_i)$ 的回归系数时，实际上是进行了一个加权最小二乘回归，其目标函数为：
$$\hat \beta(u_i, v_i) = \arg \, \, \min_{\beta(u_i, v_i)}\sum_{j = 1}^n(y_j - X_j\beta(u_i, v_i))^2w_{ij}$$
其中，$w_{ij}$ 是在位置 $(u_i, v_i)$ 处对观测点 $j$ 的权重，它是一个基于空间距离的函数。$X_j$ 是观测点 $j$ 的预测变量向量。通过最小化这个加权残差平方和，我们可以得到在位置 $(u_i, v_i)$ 处的局部回归系数估计值：
$$\hat \beta(u_i, v_i) = (X^T W(u_i, v_i) X)^{-1} X^T W(u_i, v_i) Y$$
其中，$W(u_i, v_i)$ 是一个对角矩阵，其对角线元素为 $w_{ij}$。这里的 $X$ 和 $Y$ 是全局的设计矩阵和响应变量向量。

##### Question
对于我一开始理解的 GWR, 我们能否把地理加权模型看作是一种迁移学习，他对局部模型的组合看作是辅助模型和误差项的相加？我们又为什么不使用一开始我理解的那种方式去进行建模呢

猜测 GWR 对于 $Y$ 的利用率更高，并且类迁移学习方式得到的拟合模型需要大量的计算量。

##### 补丁
学完了 SAR 之后我突然发现我一开始理解的那种建模方法反而有点像 SAR, 对不同地点的局部建模的组合很像 SAR 的空间自回归项(?)

#### 计算空间自相关性与 GWR 建模之间的关系

Moran's I 和 GWR 在空间数据分析中扮演着不同的角色，但它们之间存在密切的联系：

| 阶段             | 使用目的                                     | 使用方式                                     | 结果含义                                     |
| :--------------- | :------------------------------------------- | :------------------------------------------- | :------------------------------------------- |
| **Before GWR**   | 检查数据是否具有空间异质性或空间自相关性。 | 对 OLS 回归的残差计算 Global Moran's I。     | 帮助决定是否需要使用 GWR 或其他空间模型。如果残差存在显著空间自相关，则 GWR 可能更合适。 |
| **During GWR**   | 构建局部加权矩阵。                           | 使用空间核函数（如高斯核、双平方核）和带宽来定义 $w_{ij}$。 | 为每个局部回归提供权重，是 GWR 建模的基础。 |
| **After GWR**    | 检查 GWR 模型是否充分有效，即是否成功消除了残差中的空间自相关性。 | 对 GWR 模型的残差计算 Global Moran's I。     | 如果残差不再存在显著空间自相关，则表明 GWR 模型有效捕捉了空间异质性。 |

##### Question
Moran's I 的计算公式中出现了空间权重矩阵，而空间权重矩阵与核函数的选取方式有关，所以显然不同的核函数会带来不同的 Moran's I，这样得到的 Moran's I 在含义上有什么区别呢？
##### Answer
这个问题的关键在于理解空间权重矩阵 $W$ 的作用。Moran's I 的计算确实依赖于 $W$，而 $W$ 的构建方式（包括核函数的选择和带宽的确定）直接反映了我们对空间邻近关系的定义。不同的核函数和带宽会产生不同的 $W$ 矩阵，从而影响 Moran's I 的计算结果。这意味着：

-   **含义上的区别**：不同的 Moran's I 值反映了在特定空间邻近关系定义下（由 $W$ 决定）的空间自相关强度。例如，如果使用一个较小的带宽，Moran's I 可能更多地反映局部区域的自相关性；如果使用一个较大的带宽，则可能反映更广范围的自相关性。
-   **解释上的注意事项**：在解释 Moran's I 时，必须明确其所基于的空间权重矩阵。一个高 Moran's I 值可能在某种权重定义下成立，但在另一种权重定义下可能不成立。因此，Moran's I 的解释是相对的，取决于所采用的空间邻近关系定义。

### 空间Poisson回归模型

空间 Poisson 回归是在 Poisson 回归的基础上引入空间效应的模型，用于分析具有空间依赖性的计数数据。它结合了 Poisson 回归处理计数数据的能力和空间模型捕捉空间异质性或空间依赖性的能力。

简要概括来说，SPR 就是在 Poisson 回归的基础上加入了空间影响。传统的 Poisson 回归模型中，响应变量的对数均值与预测变量之间是全局线性关系：
$$ln(\lambda(X)) = \beta_0 + \sum_{j = 1}^p\beta_jX_j$$
而在空间 Poisson 回归中，这种关系被扩展为允许系数随空间位置变化，或者引入空间自回归项。例如，如果采用 GWR 的思想，模型可以变为：
$$ln(\lambda_i(X)) = \beta_0 (s_i)+ \sum_{j = 1}^p\beta_j(s_i)X_{ij}$$
其中，$\lambda_i(X)$ 是在位置 $s_i$ 的响应变量的条件均值，$\beta_0(s_i)$ 和 $\beta_j(s_i)$ 是在位置 $s_i$ 处变化的回归系数。这使得模型能够捕捉到不同地理位置上事件发生率与预测变量之间关系的空间异质性。

##### 空间Poisson回归的参数估计

空间 Poisson 回归模型的参数估计通常比传统 Poisson 回归更复杂，因为需要同时考虑非线性和空间依赖性。由于无法得到解析解，通常采用迭代解法来进行参数估计。常见的估计方法包括：

1.  **迭代重加权最小二乘法（IRLS）的扩展**：
    对于基于 GWR 的空间 Poisson 回归，可以在传统的 IRLS 框架内引入空间加权。在每次迭代中，根据当前参数估计值计算权重矩阵，然后进行加权最小二乘回归。这个过程会迭代进行，直到参数收敛。
    -   **初始化**：我们找到迭代的初始值，通常采用不考虑地理差异的全局 Poisson 回归的估计值作为 $\beta^{(0)}$ 和 $\mu^{(0)}$（其中 $\mu = \exp(X\beta)$）。
    -   **迭代过程**：在每次迭代中，更新权重矩阵和伪响应变量，然后进行加权最小二乘回归来更新 $\beta$。

2.  **最大似然估计（MLE）与数值优化**：
    对于更复杂的空间 Poisson 回归模型（例如包含空间自回归项的模型），可能需要直接最大化对数似然函数。这通常涉及使用数值优化算法（如 Newton-Raphson、拟牛顿法等），这些算法需要计算对数似然函数的一阶和二阶导数。

3.  **贝叶斯方法**：
    贝叶斯方法也常用于空间 Poisson 回归，尤其是在处理复杂空间结构和不确定性时。通过 MCMC（Markov Chain Monte Carlo）等方法，可以估计参数的后验分布。

### 建模目的

空间自回归模型和地理加权回归是两种处理空间数据的常用模型，但它们关注的侧重点不同。

| 模型      | 关注焦点                         | 关键特征              | 能做什么                  | 不能做什么                        |
| :------ | :--------------------------- | :---------------- | :-------------------- | :--------------------------- |
| **SAR** | 空间依赖性（Spatial Dependence）    | 因变量之间在相邻区域存在相互影响。 | 分析“一个地区的行为是否受邻居影响”。   | 假设解释变量对因变量的影响是全域一致的。         |
| **GWR** | 空间异质性（Spatial Heterogeneity） | 回归系数随地理位置变化。      | 分析“不同地方变量之间的关系是否不一样”。 | 不直接建模因变量之间的相互作用（但通过局部性间接体现）。 |

原始笔记中提到一个疑问：GWR 不建模因变量之间的相互作用，但是 GWR 建模的时候所用到的核函数难道没有体现相互作用吗？

这个疑问的解答在于对“相互作用”的理解。SAR 模型中的空间自回归项 $W Y$ 直接建模了因变量 $Y$ 在不同空间单元之间的相互影响，即一个区域的 $Y$ 值直接受到其邻居 $Y$ 值的影响。而 GWR 中的核函数 $W(u_i, v_i)$ 矩阵，其作用是根据空间距离对观测点进行加权，从而在估计局部回归系数时，给予邻近观测点更大的影响力。它体现的是**解释变量与因变量关系的空间异质性**，而不是因变量本身之间的直接相互作用。核函数是用来定义“局部”的范围和强度，它帮助 GWR 捕捉到不同地理位置上变量关系的变化，而不是因变量自身的空间溢出效应。



## Spatial Autoregressive Model

### 建模目的

空间自回归模型（Spatial Autoregressive Model, SAR）和 地理加权回归（Geographically Weighted Regression, GWR）是两种处理空间数据的常用模型，但它们关注的侧重点不同。

| 模型    | 关注焦点             | 关键特征                                   | 能做什么                                     | 不能做什么                                     |
| :------ | :------------------- | :----------------------------------------- | :------------------------------------------- | :--------------------------------------------- |
| **SAR** | 空间依赖性（Spatial Dependence） | 因变量之间在相邻区域存在相互影响。         | 分析“一个地区的行为是否受邻居影响”。        | 假设解释变量对因变量的影响是全域一致的。     |
| **GWR** | 空间异质性（Spatial Heterogeneity） | 回归系数随地理位置变化。                   | 分析“不同地方变量之间的关系是否不一样”。    | 不直接建模因变量之间的相互作用（但通过局部性间接体现）。 |

原始笔记中提到一个疑问：GWR 不建模因变量之间的相互作用，但是 GWR 建模的时候所用到的核函数难道没有体现相互作用吗？

这个疑问的解答在于对“相互作用”的理解。SAR 模型中的空间自回归项 $W Y$ 直接建模了因变量 $Y$ 在不同空间单元之间的相互影响，即一个区域的 $Y$ 值直接受到其邻居 $Y$ 值的影响。而 GWR 中的核函数 $W(u_i, v_i)$ 矩阵，其作用是根据空间距离对观测点进行加权，从而在估计局部回归系数时，给予邻近观测点更大的影响力。它体现的是**解释变量与因变量关系的空间异质性**，而不是因变量本身之间的直接相互作用。核函数是用来定义“局部”的范围和强度，它帮助 GWR 捕捉到不同地理位置上变量关系的变化，而不是因变量自身的空间溢出效应。

### SAR 的表示形式

SAR 模型通过引入一个空间滞后项来捕捉因变量的空间依赖性。其基本形式为：
$$Y =  \rho W Y + X \beta + \epsilon$$
其中：
-   $Y$ 是 $n \times 1$ 的响应变量向量。
-   $X$ 是 $n \times p$ 的预测变量矩阵。
-   $\beta$ 是 $p \times 1$ 的回归系数向量。
-   $\epsilon$ 是 $n \times 1$ 的误差项向量，通常假设服从独立同分布的正态分布。
-   $W$ 是 $n \times n$ 的空间权重矩阵，用于定义空间邻近关系。通常是行标准化的，即每行之和为 1。
-   $\rho$ 是空间自回归系数，一个常数，用于度量 $Y$ 的观测值之间空间相关的强度与方向。当 $\rho > 0$ 时，表示正空间自相关；当 $\rho < 0$ 时，表示负空间自相关。

**示例**：

假设我们有3个城市（地区），观测值为：
$$\mathbf{Y}_3 = 
\begin{bmatrix}
y_1 \\
y_2 \\
y_3
\end{bmatrix},
\quad
\mathbf{X}_3 = 
\begin{bmatrix}
x_{11} & x_{12} \\
x_{21} & x_{22} \\
x_{31} & x_{32}
\end{bmatrix},
\quad
\boldsymbol{\beta} = 
\begin{bmatrix}
\beta_1 \\
\beta_2
\end{bmatrix}$$
若空间权重矩阵为：
$$\mathbf{W} = 
\begin{bmatrix}
0 & 1 & 0 \\
1 & 0 & 1 \\
0 & 1 & 0
\end{bmatrix}$$
（即：城市1只与城市2相邻，城市2与1和3相邻，城市3只与城市2相邻）

则空间滞后项为：
$$\mathbf{W}\mathbf{Y}_3 = 
\begin{bmatrix}
0 & 1 & 0 \\
1 & 0 & 1 \\
0 & 1 & 0
\end{bmatrix}
\begin{bmatrix}
y_1 \\
y_2 \\
y_3
\end{bmatrix}
=
\begin{bmatrix}
y_2 \\
y_1 + y_3 \\
y_2
\end{bmatrix}$$
代入模型中，得到三个方程：
$$\displaylines{y_1 = \rho y_2 + \beta_1 x_{11} + \beta_2 x_{12} + \varepsilon_1 \\ y_2 = \rho (y_1 + y_3) + \beta_1 x_{21} + \beta_2 x_{22} + \varepsilon_2 \\ y_3 = \rho y_2 + \beta_1 x_{31} + \beta_2 x_{32} + \varepsilon_3}$$
可以看出：每个地区不仅仅受自己的协变量影响，还受其邻居“值”的影响。这就是空间依赖性的体现。(如果把每一个y写作回归方程的形式, 不就和我在 GWR 里提出来的对不同的地点的形式进行线性组合十分接近了吗)

**高阶空间自回归变系数模型**：

[[高阶空间自回归变系数模型的统计推断_王玉萍.pdf]]提到了高阶空间自回归变系数模型，其矩阵表示形式为（$\rho$不唯一且$\beta$与空间位置相关）：
$$Y _{i}= \rho ^T W Y + X ^T\beta + \epsilon$$
这里，$\rho$ 可以是一个向量，表示不同空间滞后阶数的系数，或者在变系数模型中，$\rho$ 和 $\beta$ 都可以是随空间位置变化的函数。空间自回归项展开为：
$$ \rho ^T W Y = (\rho_1, \ldots, \rho_r) 
\begin{pmatrix}
w_{n1,i1} & w_{n1,i2} & \cdots & w_{n1,ir} \\
w_{n2,i1} & w_{n2,i2}  & \cdots & w_{n2,ir} \\
\vdots & & & \\
w_{nr,i1} & \cdots & \cdots & w_{nr,ir}
\end{pmatrix}
\begin{pmatrix}
y_{n1} \\
y_{n2} \\
\vdots \\
y_{nr}
\end{pmatrix}$$
这表示响应变量 $Y_i$ 不仅受到其邻居 $Y_j$ 的影响，还可能受到更远邻居的影响，并且这种影响的强度和方向由 $\rho$ 向量来度量。

##### Question

为什么模型式子的左侧和右侧都出现了 $Y$，这样不会造成“循环建模”吗？另外，如果我们现在需要使用建立好的模型去预测一个新的、未知的 $Y$，那我们右侧的 $Y$ 应该怎么写？我们是否应该假设出一个 $Y$，然后通过移项去解？

##### Answer
这并非“循环建模”的错误，而是 SAR 模型捕捉空间依赖性的本质特征。为了解决这个问题，我们可以将模型进行重写：

$$Y - \rho W Y = X \beta + \epsilon$$
$$(I - \rho W) Y = X \beta + \epsilon$$
$$Y = (I - \rho W)^{-1} (X \beta + \epsilon)$$

通过这种形式，我们可以看到 $Y$ 是一个由预测变量 $X$ 和误差项 $\epsilon$ 以及空间权重矩阵 $W$ 共同决定的向量。这里的 $(I - \rho W)^{-1}$ 矩阵被称为空间乘数矩阵，它反映了局部冲击如何通过空间网络传播到其他区域。

当我们需要预测一个新的、未知的 $Y_{new}$ 时，我们不能直接使用右侧的 $Y$。预测通常分为两种情况：

1.  **内样本预测（In-sample prediction）**：如果我们要预测的是模型训练数据中的某个 $Y_i$，那么我们可以使用估计出的参数 $\hat{\rho}$ 和 $\hat{\beta}$，以及已知的 $X_i$ 和 $W$ 矩阵来计算。在这种情况下，右侧的 $Y$ 实际上是其他已知观测点的 $Y$ 值。

2.  **外样本预测（Out-of-sample prediction）**：如果我们要预测的是一个全新的、未包含在训练数据中的区域的 $Y_{new}$，这就更复杂了。通常的做法是：
    -   **假设邻居已知**：如果新区域的邻居 $Y$ 值是已知的，那么可以直接代入模型进行预测。
    -   **迭代预测**：如果邻居 $Y$ 值未知，可以采用迭代方法。首先，对所有未知区域的 $Y$ 值进行初步估计（例如，使用不含空间滞后项的模型），然后将这些初步估计值代入空间滞后项，再重新估计 $Y$，如此迭代直到收敛。这类似于求解一个大型的联立方程组。
    -   **使用简化模型**：在某些情况下，为了简化预测，可能会使用 SAR 模型的简化形式，或者在预测时忽略空间滞后项，但这会损失模型的空间捕捉能力。

### SAR 的参数估计

SAR 模型的参数估计比普通线性回归更具挑战性，主要原因在于空间滞后项 $W Y$ 与误差项 $\epsilon$ 之间存在相关性，这导致了**内生性问题（Endogeneity Problem）**。传统的最小二乘法（OLS）在这种情况下会产生有偏且不一致的估计。因此，需要采用专门的估计方法。

1.  **最大似然估计（Maximum Likelihood Estimation, MLE）**：
    MLE 是 SAR 模型参数估计最常用的方法。它假设误差项服从正态分布，并通过最大化对数似然函数来估计参数 $\rho$ 和 $\beta$。由于对数似然函数通常是非线性的，需要使用数值优化算法（如 Newton-Raphson、拟牛顿法等）进行求解。MLE 的优点是能够提供渐近有效和一致的估计量。

2.  **两阶段最小二乘法（Two-Stage Least Squares, 2SLS）或广义最小二乘法（Generalized Least Squares, GLS）**：
    这些方法属于工具变量（Instrumental Variable, IV）方法，用于解决内生性问题。其基本思想是找到与 $W Y$ 相关但与 $\epsilon$ 不相关的工具变量。常用的工具变量包括 $W X, W^2 X, \dots$ 等。2SLS 的步骤大致如下：
    -   **第一阶段**：将空间滞后项 $W Y$ 对所有外生变量（包括 $X$ 和工具变量）进行回归，得到 $W Y$ 的预测值 $\widehat{W Y}$。
    -   **第二阶段**：将 $Y$ 对 $X$ 和 $\widehat{W Y}$ 进行回归，从而得到一致的参数估计。

3.  **广义矩估计（Generalized Method of Moments, GMM）**：
    GMM 是一种更通用的估计方法，它不需要对误差项的分布做出严格假设，只需要满足一些矩条件。GMM 可以处理更复杂的内生性问题，并且在某些情况下比 MLE 更稳健。

4.  **贝叶斯方法**：
    贝叶斯方法也常用于 SAR 模型的估计，尤其是在处理小样本或复杂模型结构时。通过 MCMC 算法，可以得到参数的后验分布，从而进行推断。

**异方差问题**：

在 SAR 模型中，误差项可能存在异方差，即误差的方差不恒定。如果异方差的结构已知，可以通过适当的数据变换（例如，加权最小二乘）来处理。然而，在实际应用中，异方差的结构往往是未知的。在这种情况下，可以采用稳健标准误（Robust Standard Errors）来修正参数估计的方差，或者使用能够处理异方差的估计方法（如某些形式的 GMM）。

总之，SAR 模型的参数估计是一个复杂的问题，需要选择合适的估计方法来解决内生性和异方差等挑战。