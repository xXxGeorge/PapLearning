### Study Record 1
- 迁移学习在统计中的应用
- Poisson 回归
- 空间 Poisson 回归(地理加权 Poisson 回归)
- 空间自回归模型

### 概述
迁移学习, 是样本量不足或样本量远少于参数量的时候用辅助性样本来提高模型表现的一种方法, 这个时候直接进行回归分析会导致方差过大等不良结果. 我们不妨称呼这个这个缺少秧样本的模型为A. 所以我们寻找辅助性样本(称呼为B), 利用系数的稀疏性($l_q - sparsity$)来判断样本能否用于迁移学习, 并将原本的求A的回归模型的问题转化: 1.为求出B 的回归模型. 2. 加上B与A的误差项. 最终得到对 A 的回归模型. 其中分为两种情况, 我们是否知道哪些集合是informative auxiliary set, 对于不知道的情况我们会转化为知道的情况并解决
Possion 回归, 是一种基于 Poisson 分布的一种回归. 他对因变量的均值进行建模, 也就是对于 Y 所符合的 Poisson 分布的强度 $\lambda$ 进行建模, 认为 $\lambda$ 和自变量的线性组合之间以对数关系链接. 
地理加权 Poisson 回归, 是一种基于 GWR, 同时结合了 Poisson 回归的回归模型. 我们在原本的 Poisson 回归的基础上加入 GWR 的特征, 使得每一个参数都成为一个与取值点位置有关的函数, 使模型可以反应地理位置对于因变量的影响.
空间自回归模型, 与 GWR 相比的话, GWR 侧重模型在不同地理位置时候的不同参数取值, 而 SAR 模型只有一套参数值, 但是他的模型中含有空间自回归项, 可以体现周围的地区的因变量变化对于目标地区的影响.

### Transfer Learning
我们在这篇文章里只考虑线性回归的情况, 也即是说我们可以将形式写为矩阵形式(设计矩阵的那种)

我们发现我们需要进行回归分析的数据样本数太少了, 这时候我们可以找一个相关的研究, 其中有k个样本, 这些来自辅助模型的样本可以写作
$$
y_i^k = (x_i^k)^Tw^k + \epsilon_i^k
$$
这个时候我们就需要思考哪些辅助模型的样本是我们可以用的，也就是我们说的信息性辅助样本。
我们采取的方式是通过$l_q - sparsity$来判断的。我们构造了一个集合，可以写成如下形式L：
$$\mathbb A_q = \{1 \leq k \leq K, \quad  ||w^{(k)} - \beta||_q \leq h\}$$
现在我们知道了哪些样本可以用来辅助模型的拟合，我们分两种情况对模型进行优化：
#### Oracle Trans-Lasso
在这种情况下我们知道哪些集合是informative auxiliary set, 所以我们可使用这种算法：
step 1：
构造一个含有L1正则项的优化函数(q = 1)：
$$\hat \omega = arg\, min\{\frac{1}{2n_A}\sum_{k \in A_q}||y^k - x^k\omega^k||_2^2 +  \lambda_{\omega}||\omega^k||\}$$
其中，$\lambda_{\omega} = c_1 \sqrt{\frac{\log p}{n_A}}$.
step 2:
然后我们对其进行修正， 以满足最后的精度要求：
$$\hat \beta = \hat \omega + \hat \epsilon$$
我们直接对上面的式子进行优化计算, 也就是对其中 $\epsilon$ 的参数, 我们有
$$\hat \epsilon = arg \,\, min \{\frac{1}{2n_A}\sum_{k \in A_q}||y^k - x^kw^k  - x^k \epsilon^k||_2^2 + \lambda_w ||w^k|| + \lambda_\epsilon||\epsilon^k||\}$$
其中几个参数可以写出计算公式

##### Question
我想知道的问题是， 明明我们不知道$\beta$的取值，判断稀疏性的时候我们是怎么使用 $\mathbb A_q = \{1 \leq k \leq K, \quad  ||w^{(k)} - \beta||_q \leq h\}$ 来判断呢.
##### Answer
我的想法是可能是因为公式中是用每一个样本去计算的，这些样本显然可以计算出结果，只是容易过拟合， 方差大，所以这个方法真正的第一步其实应该是现对每一个样本点计算一个$\beta$出来，然后再对他进行进一步的修正。但是这个解释也不太对, 因为我们知道样本小的时候参数估计的方差很大, 可能导致原本不稀疏的辅助性样本被选上, 稀疏的反而被淘汰.

#### Trans-Lasso
相对于上一种情况，这次显然就是我们不知道的情况下进行估计。

我们需要计算多个estimator，然后以某种方式聚合他们，最后构造出一个模型，而这个模型理论上不会比已知稀疏度时得到的模型更差.

### Poisson Regression
#### Poisson分布的基本概念
Poisson回归是建立在Poisson分布的基础上的，故首先先简要了解一下Poisson分布。
如果X的取值大于零，且
$$P(X=k)=\frac{\lambda^k}{k!}e^{-\lambda}, \, \lambda = \overline X$$
则X满足Poisson分布。
#### Definition：Poisson Regression Model
如果我们现在已经有了一个Poisson分布的随机变量Y，我们讨论Poisson过程是在以下前提下：Y符合的Poisson分布的强度$\lambda$与X之间是对数连接的线性关系。
即
$$\displaylines {ln(\lambda(X))= \beta_0 + \sum_{i = 1}^p\beta_iX_i \\ \Rightarrow P(Y = y| X) = \frac{\lambda^k(X)e^{-\lambda(X)}}{k!}}$$

Poisson Regression 的实际意义：Poisson Distribution表示了一定时间内时间发生的次数，而Poisson Regression表示了发生次数和可能的变量之间的关系。（但是Poisson Process仍然是随机事件的发生次数）比如
- **$Y$**：某条路段一周内发生的事故次数    
- **$X$**：限速、车流量、路段长度、是否有红绿灯等
虽然有X的影响，但是否发生车祸仍然是随机的

我们会发现 Poisson 回归的形式与我们之前学习的有一点不同: 上面两种形式没有写出显式的 $\hat Y$ ,而是以概率形式出现. 但事实上,我们已经知道 $\lambda$ 是 Y 的均值, 而这个 $\lambda$ 又是在 X 的前提下得到的, 也就是说我们可以知道此时 Y 的条件期望 $E(Y|X) = exp(\beta_0 + \sum_{i =1}^p\beta_iX_i)$ ,这也正是 $\hat Y$.

##### Question
为什么我们对 $\lambda$ 建模的是 $\lambda$ 本身而不是 $E(\lambda)$?
##### Answer
这个问题的核心在于理解 Poisson 分布的随机性来源。在 Poisson 回归中，我们假设响应变量 $Y$ 在给定预测变量 $X$ 的条件下精确地服从参数为 $\lambda(X)$ 的 Poisson 分布。这里的 $\lambda(X)$ 是一个确定的值，由预测变量 $X$ 和回归系数 $\beta$ 唯一确定。事件 $Y$ 的随机性完全来源于 Poisson 分布本身的随机过程，即在已知平均发生率 $\lambda(X)$ 的情况下，实际发生次数 $Y$ 的波动。

如果我们将模型写成 $E(\lambda)$，则意味着 $\lambda$ 本身是一个随机变量，其取值过程中存在随机误差 $\epsilon$。但这与 Poisson 回归的假设不符。在 Poisson 回归中，一旦确定了 $X$ 和 $\beta$，$\lambda(X)$ 就是一个固定的、非随机的强度参数，它决定了 Poisson 过程的平均发生率。因此，我们直接对这个确定性的强度参数 $\lambda(X)$ 进行建模，而不是对其期望进行建模。
#### 计算方法：
参数估计：
- MLE
- 采用Newton-Raphson方法对$l(\beta)$球最大值也可以得到对$\beta$的最大似然估计.

### Spatial Poisson Regression
#### 空间自相关性
简而言之就是衡量空间点之间的数据是否具有相似性（~~感觉好像和迁移学习的稀疏性有点关联了~~ 其实我感觉这个描述更应该放在 SAR 里面）.空间自相关有三种情况如下：
1. **正空间自相关（Positive Spatial Autocorrelation）**
    - 邻近区域数值相似（如相邻城市的收入水平相近）
2. **负空间自相关（Negative Spatial Autocorrelation）**
    - 邻近区域数值差异大（如某城市繁华而邻接地区贫困）
3. **无空间自相关（No Spatial Autocorrelation / Random）**
    - 观测值在空间上随机分布，没有模式可言
[[地理加权泊松回归模型的模拟研究与应用_张娜娜.pdf]]提到，GWR在此前提下进行，常用的度量指标为Moran'I 指数，用于反应空间邻接或空间临近的区域单元观测值整体的相关性和差异程度。权重矩阵的取法对Moran'I的值也有影响。这里就与Moran'I有关，Moran'I 的信息如下：

#### Morans'I Value
##### Global Morans'I
用于判断整个研究区域内**是否存在**空间聚集性或空间自相关性，是一个整体评估指标。
表达式：
设观测变量为 $y_i$，其均值为 $\bar{y}$，$w_{ij}$表示第$i$个和第$j$个地区的空间权重（例如：若邻接则为 1，否则为 0），总样本数为$n$。(从表达式来看似乎Moran'I对于 GWR 的建模没有帮助，毕竟表达式钟以敬出现了$w_{ij}$)

$$I = \frac{n}{\sum_{i} \sum_{j} w_{ij}} \cdot \frac{\sum_{i} \sum_{j} w_{ij}(y_i - \bar{y})(y_j - \bar{y})}{\sum_{i} (y_i - \bar{y})^2}$$
##### Local Moran'I
Global Moran's I 只告诉你**整体空间趋势**，但不能告诉我们**具体哪个区域出现了聚集效应**。

**局部 Moran's I** 就是用于识别“局部空间异常点/热点”。

> Local Moran's I = 每个观测单位（每个城市、每个地块）的空间自相关强度

也就是对每一个空间单元 i 计算它的局部关联性。

表达式：
$$I_i = \frac{(y_i - \bar{y})}{\sum_k (y_k - \bar{y})^2} \cdot \sum_j w_{ij} (y_j - \bar{y})$$这个$I_i$表示第$i$ 个单元与其邻居在该变量上的空间相关性。

#### 地理加权模型 GWR
这里有一张表格，表示一些基本名词的意义，一开始给我带来了一些困扰

| Local Fitting                            | 局部拟合   | 以为只是前处理部分 | 其实是完整的建模方式   |
| ---------------------------------------- | ------ | --------- | ------------ |
| Global Model                             | 全局模型   | 觉得覆盖范围大   | 只表示参数不变      |
| Geographically Weighted Regression (GWR) | 地理加权回归 | 重点在于“权重”  | 实际上也做了“局部拟合” |

对于普通的多元回归，我们很难找到地理信息的关系，所以我们引入GWR，对于局部模型其形式是：
$$Y = \beta_{i0} + \sum_{k= 1}^px_{ik}\beta_{ik} + \epsilon_i$$
这个地方其实我有一个疑问，就是这样的形式和普通的对每一个区域单独进行拟合有什么区别呢，形式上完全没看出来名称中加权的特点。
经过进一步学习得知，其实在局部模型的这一块，我们就是在进行单独拟合。但是如果在此结束的话会有不可避免的缺点：数据量可能较少，使得拟合出来的模型方差过大。由此我们想到的解决方法是利用它周边的数据，根据这些数据与此处的相关程度进行组合，由此达到提高有用数据量的目的。这个判断的依据是『距离』，具体的数值由我们确定的带宽决定。
其实到这里就会让人觉得很像迁移学习了：我们利用了相似相关的数据，解决了拟合模型方差过大的问题（虽然可能不只这一个问题），只不过迁移学习是利用对辅助模型先进行拟合，然后再加上误差项的方式，而这里的地理加权模型则是用相关的模型进行组合。这就不免让人疑惑：**我们能否把地理加权模型看作是一种迁移学习，他对局部模型的组合看作是辅助模型和误差项的相加？**
说回来，我们要得到$(u_i, v_i)$处的地理加权回归的参数值就等价于求
$$\beta_i = arg \, \, min\,\,L(\beta_i) = arg \, \, min\,\,\sum_{j = 1}^n(y_i - \beta_{j0} - \sum_{k= 1}^px_{jk}\beta_{jk})^2w_{ij}$$
此处的i表示第i个地点，n表示共有n个地点，p表示回归方程中有p个变量。$w_{ij}$的意义我觉得应该是在$(u_i, v_i)$处对每一个地点的权重，这样也刚好可以组成一个$n\times n$的矩阵。
可以得到这样得到的解为
$$\hat \beta_i = X_i^T\hat \beta_i = X_i^T(X^TW_iX)^{-1}X^TW_iY$$
但是这样一来我就有一个疑问，就是按照我的理解最后对于$(u_i, v_i)$处的回归方程应该是这样：
$$y_i = \sum_{j= 1}^n(\beta_{j0} + \sum_{i = 1}^p\beta_{jm}x_{m})w_j$$
所以参数值就应该是按照下面求解：
$$\beta_i = arg \,\, min \,\,(y_i- \sum_{j= 1}^n(\beta_{j0} + \sum_{i = 1}^p\beta_{jm}x_{m})w_j)^2$$
*ANSWER*:
事实上我一开始对于局部拟合模型的理解是有问题的，我一开始以为局部拟合是一个步骤，直接按照普通的拟合方式（比如使用OLS进行优化）得到的模型是局部拟合的结果，再按照空间权重进行组合得到最后的结果；但事实上在优化的步骤中我们已经使用了空间权重，这样得到的局部拟合模型就是我们最后的结果。
新的问题是，为什么我们不用一开始的理解，即通过不同局部拟合的组合来提高数据的利用率呢（看起来有点像迁移学习）
我猜测是因为 GWR 对于 Y 的利用率更高，并且类迁移学习方式得到的拟合模型需要大量的计算量。

#### 计算空间自相关性与 GWR 建模之间的关系

| 阶段             | 使用目的          | 使用方式                  | 结果含义           |
| -------------- | ------------- | --------------------- | -------------- |
| **Before GWR** | 检查数据是否具有空间异质性 | 对 OLS 残差计算 Moran’s I  | 决定是否使用 GWR/SAR |
| **During GWR** | 构建局部加权矩阵      | 使用与 Moran’s I 相同的空间权重 | 提供建模基础         |
| **After GWR**  | 检查建模是否充分有效    | 对 GWR 残差计算 Moran’s I  | 模型评价标准         |
我们发现 Moran'I 的计算公式中出现了空间权重矩阵，而空间权重矩阵与核函数的选取方式有关，所以显然不同的核函数会带来不同的 Moran'I，这样得到的Moran'I在含义上有什么区别呢

#### 空间Poisson回归模型
简要概括来说, Spcial Poisson Regression(SPR)就是在poisson回归的基础上加入了空间影响.从[[Spatial Poisson Regression]]中的
$$ln(\lambda(X)) = \beta_0 + \sum_{i = 1}^p\beta_iX_i$$变成了
$$ln(\lambda_i(X)) = \beta_0 (s_i)+ \sum_{j = 1}^p\beta_i(s_i)X_{ij}$$
##### 空间Poisson回归的参数估计
我们只能使用迭代解法来进行参数估计. 
1. 我们找到迭代的初识值, 一般采用不考虑地理差异的数值作为 $\beta^{(0)}, \mu^{(0)}$.